{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1bDjDH7xvMZ",
        "outputId": "454e086e-e7ae-4d4f-d9a9-10ed9d0bedf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9kLLeMCwj-K"
      },
      "outputs": [],
      "source": [
        "# importing openai module into your openai environment\n",
        "import openai\n",
        "\n",
        "# assigning API KEY to initialize openai environment\n",
        "openai.api_key = ''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoKPToacx58z",
        "outputId": "bac26792-429b-4fe9-bd68-a701616c598c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKJLUTiK543X"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/train_v9.json', 'r', encoding = 'utf-8-sig') as f:\n",
        "  train_list = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/dev_v9.json', 'r', encoding = 'utf-8-sig') as f:\n",
        "  dev_list = json.load(f)\n",
        "\n",
        "total_data = train_list + dev_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm_oOilx631P"
      },
      "outputs": [],
      "source": [
        "non_referral = []\n",
        "referral = []\n",
        "\n",
        "str1 = 'سوره'\n",
        "str2 = 'نام ببرید'\n",
        "str3 = 'یکی از انواع'\n",
        "\n",
        "for qa in total_data:\n",
        "  question = qa['question']\n",
        "  if str1 not in question and str2 not in question and str3 not in question:\n",
        "    non_referral.append(qa)\n",
        "  else:\n",
        "    referral.append(qa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhOx2KE60FX0"
      },
      "source": [
        "#referential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prGoUAY97ITE"
      },
      "outputs": [],
      "source": [
        "# model=\"gpt-4o-instruct\"\n",
        "model = \"gpt-3.5-turbo-instruct\"\n",
        "\n",
        "new_list = []\n",
        "\n",
        "for qa in referral:\n",
        "  prompt = 'پاسخ پرسش'+ '\"' + qa['question'] + '\"' + 'بر اساس متن ترجمه مکارم شیرازی از آیات قرآن چیست؟ فقط پاسخ را بدون جمله کامل ارائه دهید'\n",
        "  # print(prompt)\n",
        "  response = openai.Completion.create(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        max_tokens=50,\n",
        "        n=1\n",
        "  )\n",
        "  qa['gpt'] = response.choices[0].text\n",
        "  new_list.append(qa)\n",
        "  # print(response)\n",
        "  # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkb9rGPMT6U9"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/chatgpt.json', 'w', encoding = 'utf-8-sig') as f:\n",
        "  json.dump(new_list, f, ensure_ascii = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30ArzhhJdUvH"
      },
      "source": [
        "# TEST CHATGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Itwud0Z_fuHq",
        "outputId": "077db586-25ac-4bd9-f573-9f98e90526c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q evaluate\n",
        "!pip install -q parsivar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wEzyjJ-MaJw"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "import json\n",
        "from parsivar import Normalizer\n",
        "\n",
        "my_normalizer = Normalizer()\n",
        "\n",
        "with open('/content/drive/MyDrive/chatgpt.json', 'r', encoding = 'utf-8-sig') as f:\n",
        "  data_list = json.load(f)\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for data in data_list:\n",
        "  if data['answers'][0]['text'] in data['gpt']:\n",
        "    # predictions.append(data['answers'][0]['text'])\n",
        "    # references.append(data['answers'][0]['text'])\n",
        "    predictions.append(my_normalizer.normalize(data['answers'][0]['text']))\n",
        "    references.append(my_normalizer.normalize(data['answers'][0]['text']))\n",
        "  else:\n",
        "    # predictions.append(data['gpt'].strip())\n",
        "    # references.append(data['answers'][0]['text'])\n",
        "    predictions.append(my_normalizer.normalize(data['gpt'].strip()))\n",
        "    references.append(my_normalizer.normalize(data['answers'][0]['text']))\n",
        "\n",
        "exact_match_metric = load(\"exact_match\")\n",
        "results = exact_match_metric.compute(predictions=predictions, references=references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW9xPPrMnPKR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "914qPVaxgtI2",
        "outputId": "0aa4ac9d-b520-4e82-ea61-d4477b2e958d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'exact_match': 0.06958561376075059}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbYFx67MlUK-",
        "outputId": "c8a56f91-0188-42a4-fc5d-8dbe06e5d17d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.10428085820434199"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# f1 = f1_score(ground_truth, predicted_output, average='micro')\n",
        "f1 = f1_score(references, predictions, average='weighted')\n",
        "\n",
        "f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDTNhnjuyFPl"
      },
      "source": [
        "# non_referential\n",
        "### gpt-4o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37Zd7tlByKjc"
      },
      "outputs": [],
      "source": [
        "model=\"gpt-4o\"\n",
        "# model = \"gpt-3.5-turbo-instruct\"\n",
        "\n",
        "new_list = []\n",
        "\n",
        "for qa in referral:\n",
        "  prompt = 'پاسخ پرسش'+ '\"' + qa['question'] + '\"' + 'بر اساس متن ترجمه مکارم شیرازی از آیات قرآن چیست؟ فقط پاسخ را بدون جمله کامل ارائه دهید'\n",
        "  # print(prompt)\n",
        "\n",
        "  response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=50,\n",
        "        n=1\n",
        "  )\n",
        "  qa['gpt'] = response.choices[0].message['content']\n",
        "  new_list.append(qa)\n",
        "  # print(response)\n",
        "  # break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVfJ6FRVLzNd"
      },
      "source": [
        "### gpt-3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS_sd2hbNI43",
        "outputId": "122471b2-4437-4b39-d8fb-6cd13e8dba15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5078"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(non_referral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKJFz2EJ9AJI"
      },
      "outputs": [],
      "source": [
        "# model=\"gpt-4o-instruct\"\n",
        "model = \"gpt-3.5-turbo-instruct\"\n",
        "\n",
        "new_list2 = []\n",
        "\n",
        "for qa in non_referral[2500:]:\n",
        "  prompt = ' طبق آیات قرآن' + qa['question'] + ' پاسخ را به‌صورت کوتاه و بدون جمله کامل از ترجمه مکارم شیرازی استخراج کنید'\n",
        "  # prompt = 'پاسخ پرسش'+ '\"' + qa['question'] + '\"' + 'بر اساس متن ترجمه مکارم شیرازی از آیات قرآن چیست؟ فقط پاسخ را بدون جمله کامل ارائه دهید'\n",
        "  # print(prompt)\n",
        "  response = openai.Completion.create(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        max_tokens=50,\n",
        "        n=1\n",
        "  )\n",
        "  qa['gpt'] = response.choices[0].text\n",
        "  new_list2.append(qa)\n",
        "  # print(qa['gpt'])\n",
        "  # break\n",
        "\n",
        "with open('/content/drive/MyDrive/chatgpt_non_referential_part2.json', 'w', encoding = 'utf-8-sig') as f:\n",
        "  json.dump(new_list2, f, ensure_ascii = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYWSgjAgWzRw"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlHRjvIIWyp6",
        "outputId": "e0b27c88-113d-493a-95a2-9fafdffff882"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'exact_match': 0.06794013391098858}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from evaluate import load\n",
        "import json\n",
        "from parsivar import Normalizer\n",
        "\n",
        "my_normalizer = Normalizer()\n",
        "\n",
        "with open('/content/drive/MyDrive/chatgpt_non_referential_part1.json', 'r', encoding = 'utf-8-sig') as f:\n",
        "  data_list1 = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/chatgpt_non_referential_part2.json', 'r', encoding = 'utf-8-sig') as f:\n",
        "  data_list2 = json.load(f)\n",
        "\n",
        "data_list = data_list1 + data_list2\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for data in data_list:\n",
        "  if data['answers'][0]['text'] in data['gpt']:\n",
        "    # predictions.append(data['answers'][0]['text'])\n",
        "    # references.append(data['answers'][0]['text'])\n",
        "    predictions.append(my_normalizer.normalize(data['answers'][0]['text']))\n",
        "    references.append(my_normalizer.normalize(data['answers'][0]['text']))\n",
        "  else:\n",
        "    # predictions.append(data['gpt'].strip())\n",
        "    # references.append(data['answers'][0]['text'])\n",
        "    predictions.append(my_normalizer.normalize(data['gpt'].strip()))\n",
        "    references.append(my_normalizer.normalize(data['answers'][0]['text']))\n",
        "\n",
        "exact_match_metric = load(\"exact_match\")\n",
        "results = exact_match_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cACnUnFyZMXp",
        "outputId": "248fa190-4f80-47dd-f80b-486170860d12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.10486502786647738"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# f1 = f1_score(ground_truth, predicted_output, average='micro')\n",
        "f1 = f1_score(references, predictions, average='weighted')\n",
        "\n",
        "f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GT90Vjky8og"
      },
      "outputs": [],
      "source": [
        "def ask_chat_gpt(question):\n",
        "\n",
        "    #Generate a response from ChatGPT using the question chat message\n",
        "    response = openai.ChatCompletion.create(\n",
        "    model = model,\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ],\n",
        "    max_tokens=100,\n",
        "    temperature=0.9,\n",
        "    frequency_penalty=0.5,\n",
        "    presence_penalty=0.5,\n",
        "    )\n",
        "\n",
        "    #Extract the answer from the response\n",
        "    answer = response.choices[0].message['content']\n",
        "\n",
        "    #Return the answer\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSJrChWEDxWD",
        "outputId": "e9b5ae99-a68b-4bdb-d2ab-d8c9ef352dda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"id\": \"cmpl-9SVMz609KRymt1puG7db0gTvwGdfo\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"created\": 1716580797,\n",
            "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"text\": \"\\\"\\n\\n\\\"\\u0634\\u0645\\u0633\\\"\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 114,\n",
            "    \"completion_tokens\": 7,\n",
            "    \"total_tokens\": 121\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "prompt = 'پاسخ پرسش'+ '\"' + referral[0]['question'] + '\"' + 'بر اساس متن ترجمه مکارم شیرازی از آیات قرآن چیست؟ فقط پاسخ را بدون جمله کامل ارائه دهید'\n",
        "  # print(prompt)\n",
        "response = openai.Completion.create(\n",
        "      model=\"gpt-3.5-turbo-instruct\",\n",
        "      prompt=prompt,\n",
        "      max_tokens=50,\n",
        "      n=1,\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCwQKyLgsLUc",
        "outputId": "b1e7b81c-2027-4466-bd54-1138b65897c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"\n",
            "\n",
            "\"شمس\"\n"
          ]
        }
      ],
      "source": [
        "generated_text = response.choices[0].text\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toHO2jNNtTN4",
        "outputId": "711077dd-3327-463d-a8d9-ae870c8a8065"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'در آیه 113 سوره توبه به کدام دسته از انسان\\u200cها اشاره شده است؟',\n",
              " 'pq_id': 's9.113',\n",
              " 'context': 'برای پیامبر و مؤمنان، شایسته نبود که برای مشرکان طلب آمرزش کنند، هر چند از نزدیکانشان باشند؛ پس از آنکه بر آنها روشن شد که این گروه، اهل دوزخند!',\n",
              " 'answers': [{'text': 'مؤمنان', 'start_char': 14}]}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "referral[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPv_QC4oFXYp"
      },
      "outputs": [],
      "source": [
        "!pip install -q OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHy2WgVnHVOb"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main interaction loop for the chatbot.\n",
        "    \"\"\"\n",
        "    print(\"Welcome to Chatbot! Type 'quit' to exit.\")\n",
        "\n",
        "    user_input = \"\"\n",
        "    while user_input.lower() != \"quit\":\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() != \"quit\":\n",
        "            response = chat_with_openai(user_input)  # Pass user_input as an argument\n",
        "            print(f\"Chatbot: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "EIVbD1gFHOaD",
        "outputId": "140da794-2ce6-4146-922b-27a7b8ba147b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to Chatbot! Type 'quit' to exit.\n",
            "You: \n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'openai' has no attribute 'chat'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ad35dc9345b0>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-f77edbab0f2c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_with_openai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass user_input as an argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Chatbot: {response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-ad35dc9345b0>\u001b[0m in \u001b[0;36mchat_with_openai\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      8\u001b[0m     }\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     response = openai.chat.completions.create(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'chat'"
          ]
        }
      ],
      "source": [
        "def chat_with_openai(prompt):\n",
        "    \"\"\"\n",
        "    Sends the prompt to OpenAI API using the chat interface and gets the model's response.\n",
        "    \"\"\"\n",
        "    message = {\n",
        "        'role': 'user',\n",
        "        'content': prompt\n",
        "    }\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[message]\n",
        "    )\n",
        "\n",
        "    # Extract the chatbot's message from the response.\n",
        "    # Assuming there's at least one response and taking the last one as the chatbot's reply.\n",
        "    chatbot_response = response.choices[0].message['content']\n",
        "    return chatbot_response.strip()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "jxFa3mHDFOBv",
        "outputId": "1e825ea1-d568-42c1-d24d-cff24623ba8e"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'OpenAI' from 'openai' (/usr/local/lib/python3.10/dist-packages/openai/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a192bf6d02ec>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m response = client.chat.completions.create(\n\u001b[1;32m      5\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo-0125\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'OpenAI' from 'openai' (/usr/local/lib/python3.10/dist-packages/openai/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-0125\",\n",
        "  response_format={ \"type\": \"json_object\" },\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
        "  ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbMygc_sJlMv"
      },
      "source": [
        "save referential and non-referential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IscBZ20fJNEg"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/train_v9.json', 'r', encoding = 'utf-8-sig') as f:\n",
        "  train_list = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/dev_v9.json', 'r', encoding = 'utf-8-sig') as f:\n",
        "  dev_list = json.load(f)\n",
        "\n",
        "total_data = train_list + dev_list\n",
        "\n",
        "non_referral = []\n",
        "referral = []\n",
        "\n",
        "str1 = 'سوره'\n",
        "str2 = 'نام ببرید'\n",
        "str3 = 'یکی از انواع'\n",
        "\n",
        "for qa in total_data:\n",
        "  question = qa['question']\n",
        "  if str1 not in question and str2 not in question and str3 not in question:\n",
        "    non_referral.append(qa)\n",
        "  else:\n",
        "    referral.append(qa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFjMdBaKJ392"
      },
      "outputs": [],
      "source": [
        "for i in range(len(referral)):\n",
        "  prompt = 'پاسخ پرسش'+ '\"' + referral[i]['question'] + '\"' + 'بر اساس متن ترجمه مکارم شیرازی از آیات قرآن چیست؟ فقط پاسخ را بدون جمله کامل ارائه دهید'\n",
        "  referral[i]['prompt'] = prompt\n",
        "\n",
        "with open('content/drive/MyDrive/referential.json', 'w', encoding = 'utf-8-sig') as f:\n",
        "  f.dump(referral, f, ensure_ac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBwfXLAXpx_B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ha3Y7IETpx3P",
        "outputId": "eabd12b0-982a-4232-c3df-5700aecaec75"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'..................................\\n موسی از ظرف راست کوه طور فراخوانی شد .'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str = \"\\n\\n ..................................\\nموسی از ظرف راست کوه طور فراخوانی شد.\\n\"\n",
        "\n",
        "str2 = my_normalizer.normalize(str.strip())\n",
        "str2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
