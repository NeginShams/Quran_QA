{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXzjxRbzDPsz",
        "outputId": "466c3730-89e5-47e0-c0ec-2e5117618cb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "installing Pyserini library for sparse passage retrieval"
      ],
      "metadata": {
        "id": "Z9d---UvGNUG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q_jhVb7XEhTH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pyserini==0.22.0\n",
        "!pip install faiss-cpu\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "read pquad dataset"
      ],
      "metadata": {
        "id": "7Y_1WxuwengO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzT2USKCETSd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/PQuAD-main/Dataset/Train.json', 'r') as f:\n",
        "  train_data = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/PQuAD-main/Dataset/Validation.json', 'r') as f:\n",
        "  dev_data = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/PQuAD-main/Dataset/Test.json', 'r') as f:\n",
        "  test_data = json.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a function to write documents with json-lines format"
      ],
      "metadata": {
        "id": "X89dK0mUer4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dump_jsonl(data, output_path, append=False):\n",
        "    \"\"\"\n",
        "    Write list of objects to a JSON lines file.\n",
        "    \"\"\"\n",
        "    mode = 'a+' if append else 'w'\n",
        "    with open(output_path, mode, encoding='utf-8') as f:\n",
        "        for line in data:\n",
        "            json_record = json.dumps(line, ensure_ascii=False)\n",
        "            f.write(json_record + '\\n')\n",
        "    print('Wrote {} records to {}'.format(len(data), output_path))"
      ],
      "metadata": {
        "id": "S0LH0tz5Mc9H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "index the passages of pquad dataset"
      ],
      "metadata": {
        "id": "gurbYNO7ezXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "i = 0\n",
        "\n",
        "for data in train_data['data']:\n",
        "  for paragraph in data['paragraphs']:\n",
        "    doc_info = {}\n",
        "    doc_info['id'] = i\n",
        "    doc_info['contents'] = paragraph['context']\n",
        "    doc_info['title'] = data['title']\n",
        "    i += 1\n",
        "    documents.append(doc_info)\n",
        "\n",
        "for data in dev_data['data']:\n",
        "  for paragraph in data['paragraphs']:\n",
        "    doc_info = {}\n",
        "    doc_info['id'] = i\n",
        "    doc_info['contents'] = paragraph['context']\n",
        "    doc_info['title'] = data['title']\n",
        "    i += 1\n",
        "    documents.append(doc_info)\n",
        "\n",
        "for data in test_data['data']:\n",
        "  for paragraph in data['paragraphs']:\n",
        "    doc_info = {}\n",
        "    doc_info['id'] = i\n",
        "    doc_info['contents'] = paragraph['context']\n",
        "    doc_info['title'] = data['title']\n",
        "    i += 1\n",
        "    documents.append(doc_info)\n",
        "\n",
        "output_path = '/content/drive/MyDrive/pquad_documents.jsonl'\n",
        "dump_jsonl(documents, output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Te_M8BwHVDX",
        "outputId": "82b8d157-bfd3-4be1-b36d-885d0952d0a7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote 11141 records to /content/drive/MyDrive/pquad_documents.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m pyserini.index.lucene \\\n",
        "  --collection JsonCollection \\\n",
        "  --input /content/drive/MyDrive/tests/resources/pquad_collection_jsonl \\\n",
        "  --language fa \\\n",
        "  --index /content/drive/MyDrive/indexes/pquad_collection_jsonl \\\n",
        "  --generator DefaultLuceneDocumentGenerator \\\n",
        "  --threads 1 \\\n",
        "  --storePositions --storeDocvectors --storeRaw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEkaiwVJM_wk",
        "outputId": "721954c2-7de9-48be-9c2e-5d57c400669b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
            "2024-01-22 22:45:03,645 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
            "2024-01-22 22:45:03,647 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
            "2024-01-22 22:45:03,647 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
            "2024-01-22 22:45:03,648 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: /content/drive/MyDrive/tests/resources/pquad_collection_jsonl\n",
            "2024-01-22 22:45:03,648 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
            "2024-01-22 22:45:03,649 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
            "2024-01-22 22:45:03,649 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 1\n",
            "2024-01-22 22:45:03,650 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: fa\n",
            "2024-01-22 22:45:03,650 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
            "2024-01-22 22:45:03,650 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
            "2024-01-22 22:45:03,651 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
            "2024-01-22 22:45:03,651 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
            "2024-01-22 22:45:03,651 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
            "2024-01-22 22:45:03,652 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
            "2024-01-22 22:45:03,652 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
            "2024-01-22 22:45:03,653 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
            "2024-01-22 22:45:03,653 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
            "2024-01-22 22:45:03,653 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
            "2024-01-22 22:45:03,655 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: false\n",
            "2024-01-22 22:45:03,655 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: /content/drive/MyDrive/indexes/pquad_collection_jsonl\n",
            "2024-01-22 22:45:03,659 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
            "2024-01-22 22:45:03,680 INFO  [main] index.IndexCollection (IndexCollection.java:459) - Using language-specific analyzer\n",
            "2024-01-22 22:45:03,680 INFO  [main] index.IndexCollection (IndexCollection.java:460) - Language: fa\n",
            "2024-01-22 22:45:03,973 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 1 threads initialized.\n",
            "2024-01-22 22:45:03,973 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in /content/drive/MyDrive/tests/resources/pquad_collection_jsonl\n",
            "2024-01-22 22:45:03,978 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
            "2024-01-22 22:45:03,979 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
            "2024-01-22 22:45:11,098 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - pquad_collection_jsonl/pquad_documents.jsonl: 11141 docs added.\n",
            "2024-01-22 22:45:13,885 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 11,141 documents indexed\n",
            "2024-01-22 22:45:13,887 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
            "2024-01-22 22:45:13,887 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:           11,141\n",
            "2024-01-22 22:45:13,888 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
            "2024-01-22 22:45:13,889 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
            "2024-01-22 22:45:13,889 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
            "2024-01-22 22:45:13,889 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
            "2024-01-22 22:45:13,896 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 11,141 documents indexed in 00:00:10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train set"
      ],
      "metadata": {
        "id": "SxzY3WSCe6aX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Txn_8NceFIA1"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from pyserini.search.lucene import LuceneSearcher\n",
        "\n",
        "\n",
        "searcher = LuceneSearcher('/content/drive/MyDrive/indexes/pquad_collection_jsonl')\n",
        "searcher.set_language('fa')\n",
        "\n",
        "dpr_data_list = []\n",
        "\n",
        "for data in train_data['data']:\n",
        "  for paragraph in data['paragraphs']:\n",
        "    for qa in paragraph['qas']:\n",
        "      if not qa['is_impossible'] and len(qa['answers']) == 1:\n",
        "        dpr_data = {}\n",
        "        dpr_data['dataset'] = 'PQuAD'\n",
        "        dpr_data['question'] = qa['question']\n",
        "        dpr_data['answers'] = qa['answers']\n",
        "        dpr_data['positive_ctxs'] = [{'title': data['title'] , 'text': paragraph['context'],\n",
        "                                'score': 1000, 'title_score':1, 'passage_id':qa['id']}]\n",
        "        dpr_data['negative_ctxs'] = []\n",
        "\n",
        "        hits = searcher.search(qa['question'])\n",
        "        hard_negatives = []\n",
        "\n",
        "        for i in range(len(hits)):\n",
        "          negative_data = {}\n",
        "          negative_id = hits[i].docid\n",
        "          doc = searcher.doc(hits[0].docid)\n",
        "          doc_dict = json.loads(doc.raw())\n",
        "          negative_content = doc_dict['contents']\n",
        "          negative_title = doc_dict['title']\n",
        "          if qa['answers'][0]['text'] not in negative_content and data['title'] != negative_title:\n",
        "            negative_data['passage_id'] = negative_id\n",
        "            negative_data['text'] = negative_content\n",
        "            negative_data['title'] = negative_title\n",
        "            negative_data['score'] = hits[i].score\n",
        "            negative_data['title_score'] = 0\n",
        "            hard_negatives.append(negative_data)\n",
        "\n",
        "          if len(hard_negatives) == 5:\n",
        "            break\n",
        "\n",
        "        dpr_data['hard_negative_ctxs'] = hard_negatives\n",
        "\n",
        "        if len(dpr_data['hard_negative_ctxs']) == 5:\n",
        "          dpr_data_list.append(dpr_data)\n",
        "\n",
        "        del dpr_data, hard_negatives, hits\n",
        "  # gc.collect()\n",
        "\n",
        "random.shuffle(dpr_data_list)\n",
        "with open('/content/drive/MyDrive/PQuAD_DPR_train.json', 'w') as fout:\n",
        "  json.dump(dpr_data_list, fout, ensure_ascii = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "development set"
      ],
      "metadata": {
        "id": "x2W3KfuEe8vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from pyserini.search.lucene import LuceneSearcher\n",
        "\n",
        "\n",
        "searcher = LuceneSearcher('/content/drive/MyDrive/indexes/pquad_collection_jsonl')\n",
        "searcher.set_language('fa')\n",
        "\n",
        "dpr_data_list = []\n",
        "\n",
        "my_data = dev_data['data'] + test_data['data']\n",
        "\n",
        "for data in my_data:\n",
        "  for paragraph in data['paragraphs']:\n",
        "    for qa in paragraph['qas']:\n",
        "      if not qa['is_impossible'] and len(qa['answers']) == 1:\n",
        "        dpr_data = {}\n",
        "        dpr_data['dataset'] = 'PQuAD'\n",
        "        dpr_data['question'] = qa['question']\n",
        "        dpr_data['answers'] = qa['answers']\n",
        "        dpr_data['positive_ctxs'] = [{'title': data['title'] , 'text': paragraph['context'],\n",
        "                                'score': 1000, 'title_score':1, 'passage_id':qa['id']}]\n",
        "        dpr_data['negative_ctxs'] = []\n",
        "\n",
        "        hits = searcher.search(qa['question'])\n",
        "        hard_negatives = []\n",
        "\n",
        "        for i in range(len(hits)):\n",
        "          negative_data = {}\n",
        "          negative_id = hits[i].docid\n",
        "          doc = searcher.doc(hits[0].docid)\n",
        "          doc_dict = json.loads(doc.raw())\n",
        "          negative_content = doc_dict['contents']\n",
        "          negative_title = doc_dict['title']\n",
        "          if qa['answers'][0]['text'] not in negative_content and data['title'] != negative_title:\n",
        "            negative_data['passage_id'] = negative_id\n",
        "            negative_data['text'] = negative_content\n",
        "            negative_data['title'] = negative_title\n",
        "            negative_data['score'] = hits[i].score\n",
        "            negative_data['title_score'] = 0\n",
        "            hard_negatives.append(negative_data)\n",
        "\n",
        "          if len(hard_negatives) == 5:\n",
        "            break\n",
        "\n",
        "        dpr_data['hard_negative_ctxs'] = hard_negatives\n",
        "\n",
        "        if len(dpr_data['hard_negative_ctxs']) == 5:\n",
        "          dpr_data_list.append(dpr_data)\n",
        "\n",
        "        del dpr_data, hard_negatives, hits\n",
        "  # gc.collect()\n",
        "\n",
        "random.shuffle(dpr_data_list)\n",
        "with open('/content/drive/MyDrive/PQuAD_DPR_dev.json', 'w') as fout:\n",
        "  json.dump(dpr_data_list, fout, ensure_ascii = False)"
      ],
      "metadata": {
        "id": "JFFBNG_ccKXO"
      },
      "execution_count": 21,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}