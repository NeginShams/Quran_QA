{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4fUkJ8NLyHtJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6xRkuBjYlS2",
        "outputId": "67a05082-5b1a-4497-faa4-88568b51ae31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert multiverse to singleverse"
      ],
      "metadata": {
        "id": "PVYtHdDvWwFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/total_temp.jsonl', 'r',encoding=\"utf8\") as json_file:\n",
        "    json_list = list(json_file)\n",
        "\n",
        "with open('/content/drive/MyDrive/makarem.xml', 'r', encoding=\"utf8\") as f:\n",
        "  content = f.read()\n",
        "\n",
        "soup= BeautifulSoup(content, 'xml')\n",
        "\n",
        "def findOccurrences(s, ch):\n",
        "    return [i for i, letter in enumerate(s) if letter == ch]\n",
        "\n",
        "dict_data_list = []\n",
        "exception_list = []\n",
        "\n",
        "for json_str in json_list:\n",
        "\n",
        "    result = json.loads(json_str)\n",
        "    context = result['context']\n",
        "    context = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", context)\n",
        "    context = re.sub(\" +\", \" \", context)\n",
        "    answer = result['answers']\n",
        "    answer_freq = context.count(answer)\n",
        "\n",
        "    new_data = {}\n",
        "    new_data['question'] = result['question']\n",
        "    answer_dict = {}\n",
        "\n",
        "    if 's' in result['pq_id']:\n",
        "        new_data['pq_id'] = result['pq_id']\n",
        "\n",
        "        if '*' in context:\n",
        "            markers = findOccurrences(context, '*')\n",
        "            answer_dict['text'] = result['answers']\n",
        "            answer_dict['start_char'] = markers[0]\n",
        "            new_data['answers'] = [answer_dict]\n",
        "            new_data['context'] = context.replace('*', '')\n",
        "        else:\n",
        "            new_data['context'] = context\n",
        "            answer_dict['text'] = result['answers']\n",
        "            answer_dict['start_char'] = context.find(result['answers'])\n",
        "            new_data['answers'] = [answer_dict]\n",
        "\n",
        "    elif 's' not in result['pq_id']:\n",
        "        verses = (result['pq_id'].split(\":\", 1)[1]).split('_')[0]\n",
        "        start_verse = int(verses.split('-')[0])\n",
        "        end_verse = int(verses.split('-')[1])\n",
        "        chapter = result['pq_id'].split(\":\", 1)[0]\n",
        "\n",
        "        if start_verse == end_verse:\n",
        "            new_data['pq_id'] = 's'+ str(chapter) + '.' + str(start_verse)\n",
        "\n",
        "            if '*' in context:\n",
        "                markers = findOccurrences(context, '*')\n",
        "                answer_dict['text'] = result['answers']\n",
        "                answer_dict['start_char'] = markers[0]\n",
        "                new_data['answers'] = [answer_dict]\n",
        "                new_data['context'] = context.replace('*', '')\n",
        "            else:\n",
        "                new_data['context'] = context\n",
        "                answer_dict['text'] = result['answers']\n",
        "                answer_dict['start_char'] = context.find(result['answers'])\n",
        "                new_data['answers'] = [answer_dict]\n",
        "\n",
        "        else:\n",
        "            verse_list = []\n",
        "            for i in range(start_verse, end_verse+1):\n",
        "                verse_dict = {}\n",
        "                verse_id = 's'+ str(chapter) + '.' + str(i)\n",
        "                try:\n",
        "                  verse_content = soup.find(id=verse_id).contents[0]\n",
        "                  verse_content = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", verse_content)\n",
        "                  verse_content = re.sub(\" +\", \" \", verse_content)\n",
        "                except:\n",
        "                  print('verse_id: ' + str(verse_id) + 'pq_id: ' + str(result['pq_id']))\n",
        "\n",
        "                verse_dict['id'] = verse_id\n",
        "                verse_dict['content'] = verse_content\n",
        "                verse_list.append(verse_dict)\n",
        "\n",
        "            if '*' in context:\n",
        "                counter = 0\n",
        "                id = ''\n",
        "                new_content = ''\n",
        "\n",
        "                clean_verse = context.replace('*', '')\n",
        "                for verse in verse_list:\n",
        "                    if verse['content'] in context:\n",
        "                        counter+=1\n",
        "                    else:\n",
        "                        id = verse['id']\n",
        "                        new_content = verse['content']\n",
        "\n",
        "\n",
        "                if counter == len(verse_list)-1 and id:\n",
        "                    new_data['pq_id'] = id\n",
        "                    new_data['context'] = new_content\n",
        "                    if new_content.count(result['answers']) == 1:\n",
        "                        start_char = new_content.find(result['answers'])\n",
        "                        answer_dict['text'] = result['answers']\n",
        "                        answer_dict['start_char'] = new_content.find(result['answers'])\n",
        "                        new_data['answers'] = [answer_dict]\n",
        "                    else:\n",
        "                        for verse in verse_list:\n",
        "                            if verse['id'] != id:\n",
        "                                context = context.replace(verse['content'], '')\n",
        "\n",
        "                        markers = findOccurrences(context, '*')\n",
        "                        answer_dict['text'] = result['answers']\n",
        "                        answer_dict['start_char'] = markers[0]\n",
        "                        new_data['answers'] = [answer_dict]\n",
        "                        new_data['context'] = context.replace('*', '')\n",
        "\n",
        "                else:\n",
        "                  pass\n",
        "                    # print(result['pq_id'])\n",
        "                    # exception_list.append(result)\n",
        "                # markers = findOccurrences(context, '*')\n",
        "                # passage = context.replace('*', '')\n",
        "\n",
        "            else:\n",
        "                for verse in verse_list:\n",
        "                    if answer in verse['content']:\n",
        "                        new_data['pq_id'] = verse['id']\n",
        "                        new_data['context'] = verse['content']\n",
        "                        answer_dict['text'] = result['answers']\n",
        "                        answer_dict['start_char'] = verse['content'].find(result['answers'])\n",
        "                        new_data['answers'] = [answer_dict]\n",
        "                        break\n",
        "\n",
        "\n",
        "    if 'answers' in new_data.keys():\n",
        "        dict_data_list.append(new_data)\n",
        "    else:\n",
        "        exception_list.append(result)\n"
      ],
      "metadata": {
        "id": "aGm_ZBKPYO07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(exception_list)"
      ],
      "metadata": {
        "id": "y2ig_sc9ZWsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7cc127-ed4d-4a3a-d7a1-114fb917e455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dict_data_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQjAms6Q9iwC",
        "outputId": "9dd3975e-b9b8-4d82-9e7a-9f27f35ea489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4755"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import math\n",
        "\n",
        "random.shuffle(dict_data_list)\n",
        "split_per = math.trunc(0.8 * len(dict_data_list))\n",
        "\n",
        "with open('/content/drive/MyDrive/train_v5.json', 'w', encoding = 'utf_8_sig') as fp:\n",
        "  json.dump(dict_data_list[:split_per], fp, ensure_ascii = False)\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/dev_v5.json', 'w', encoding = 'utf_8_sig') as fout:\n",
        "  json.dump(dict_data_list[split_per:], fout, ensure_ascii = False)"
      ],
      "metadata": {
        "id": "Vs_S8d6F9mrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save dataset with squad format"
      ],
      "metadata": {
        "id": "nenPPlyRW5xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_dict_data_list = []\n",
        "for data in dict_data_list:\n",
        "  new_data = {}\n",
        "  new_data['title'] = ''\n",
        "  # try:\n",
        "  new_data['paragraphs'] = [{'context': data['context'], 'qas':[{'answers':[{'answer_start':data['answers'][0]['start_char'], 'text':data['answers'][0]['text']}],'id':data['pq_id'], 'question': data['question'], 'is_impossible': False}]}]\n",
        "  # except:\n",
        "    # print(data)\n",
        "  new_dict_data_list.append(new_data)\n",
        "\n",
        "final_data_train = {}\n",
        "final_data_dev = {}\n",
        "\n",
        "final_data_train['data'] = new_dict_data_list[:split_per]\n",
        "final_data_dev['data'] = new_dict_data_list[split_per:]\n",
        "\n",
        "with open('/content/drive/MyDrive/train_SQuAD_format_v5.json', 'w') as fp:\n",
        "  json.dump(final_data_train, fp, ensure_ascii = False)\n",
        "\n",
        "with open('/content/drive/MyDrive/dev_SQuAD_format_v5.json', 'w') as fp:\n",
        "  json.dump(final_data_dev, fp, ensure_ascii = False)"
      ],
      "metadata": {
        "id": "NeSyldTR92oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DPR dataset creation"
      ],
      "metadata": {
        "id": "m4jZ08RfXArF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyserini==0.22.0\n",
        "!pip install faiss-cpu\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
      ],
      "metadata": {
        "id": "m-NEwIfTGAQp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyserini.search.lucene import LuceneSearcher\n",
        "\n",
        "searcher = LuceneSearcher('/content/drive/MyDrive/indexes/sample_collection_jsonl')\n",
        "searcher.set_language('fa')\n",
        "hits = searcher.search('لات')\n",
        "\n",
        "for i in range(len(hits)):\n",
        "    print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_3PhuzFwvUP",
        "outputId": "022a1976-ab61-4e17-a74a-3c23b46541fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1 s53.19 4.84050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/train_v5.json', 'r', encoding = 'utf-8-sig') as f:\n",
        "  dict_data_list = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/makarem.xml', 'r', encoding=\"utf8\") as f:\n",
        "  content = f.read()\n",
        "\n",
        "soup= BeautifulSoup(content, 'xml')\n",
        "\n",
        "searcher = LuceneSearcher('/content/drive/MyDrive/indexes/sample_collection_jsonl')\n",
        "searcher.set_language('fa')\n",
        "\n",
        "dpr_data_list = []\n",
        "\n",
        "for qa in dict_data_list:\n",
        "  question = qa['question']\n",
        "  dpr_data = {}\n",
        "  dpr_data['dataset'] = 'Quran_QA'\n",
        "  dpr_data['question'] = qa['question']\n",
        "  dpr_data['answers'] = qa['answers']\n",
        "  dpr_data['positive_ctxs'] = [{'title': '' , 'text': qa['context'],\n",
        "                                'score': 1000, 'title_score':1, 'passage_id':qa['pq_id']}]\n",
        "  dpr_data['negative_ctxs'] = []\n",
        "\n",
        "  hits = searcher.search(question)\n",
        "  hard_negatives = []\n",
        "  # if len(hits)>5:\n",
        "  #   negatives_number = 5\n",
        "  # else:\n",
        "  #   negatives_number = len(hits)\n",
        "\n",
        "  for i in range(len(hits)):\n",
        "    verse_data = {}\n",
        "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
        "    verse_id = hits[i].docid\n",
        "    verse_content = soup.find(id=verse_id).contents[0]\n",
        "    if qa['answers'][0]['text'] not in verse_content:\n",
        "      verse_data['passage_id'] = verse_id\n",
        "      verse_data['text'] = verse_content\n",
        "      verse_data['title'] = ''\n",
        "      verse_data['score'] = hits[i].score\n",
        "      verse_data['title_score'] = 0\n",
        "      hard_negatives.append(verse_data)\n",
        "\n",
        "    if len(hard_negatives) == 5:\n",
        "      break\n",
        "\n",
        "  dpr_data['hard_negative_ctxs'] = hard_negatives\n",
        "\n",
        "  dpr_data_list.append(dpr_data)\n",
        "\n",
        "  del dpr_data, hard_negatives, hits\n"
      ],
      "metadata": {
        "id": "wvi-Ghnbys78"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dpr_data_list)"
      ],
      "metadata": {
        "id": "vowBqTsJy__m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef00036-d165-4481-bba3-0a6f74656aab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3804"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/DPR_train_v2.json', 'w') as fout:\n",
        "  json.dump(dpr_data_list, fout, ensure_ascii = False)"
      ],
      "metadata": {
        "id": "VAGqhpKuLWIO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/dev_v5.json', 'r', encoding = 'utf-8-sig') as f:\n",
        "  dict_data_list = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/makarem.xml', 'r', encoding=\"utf8\") as f:\n",
        "  content = f.read()\n",
        "\n",
        "soup= BeautifulSoup(content, 'xml')\n",
        "\n",
        "searcher = LuceneSearcher('/content/drive/MyDrive/indexes/sample_collection_jsonl')\n",
        "searcher.set_language('fa')\n",
        "\n",
        "dpr_data_list = []\n",
        "\n",
        "for qa in dict_data_list:\n",
        "  question = qa['question']\n",
        "  dpr_data = {}\n",
        "  dpr_data['dataset'] = 'Quran_QA'\n",
        "  dpr_data['question'] = qa['question']\n",
        "  dpr_data['answers'] = qa['answers']\n",
        "  dpr_data['positive_ctxs'] = [{'title': '' , 'text': qa['context'],\n",
        "                                'score': 1000, 'title_score':1, 'passage_id':qa['pq_id']}]\n",
        "  dpr_data['negative_ctxs'] = []\n",
        "\n",
        "  hits = searcher.search(question)\n",
        "  hard_negatives = []\n",
        "  # if len(hits)>5:\n",
        "  #   negatives_number = 5\n",
        "  # else:\n",
        "  #   negatives_number = len(hits)\n",
        "\n",
        "  for i in range(len(hits)):\n",
        "    verse_data = {}\n",
        "    # print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f}')\n",
        "    verse_id = hits[i].docid\n",
        "    verse_content = soup.find(id=verse_id).contents[0]\n",
        "    if qa['answers'][0]['text'] not in verse_content:\n",
        "      verse_data['passage_id'] = verse_id\n",
        "      verse_data['text'] = verse_content\n",
        "      verse_data['title'] = ''\n",
        "      verse_data['score'] = hits[i].score\n",
        "      verse_data['title_score'] = 0\n",
        "      hard_negatives.append(verse_data)\n",
        "\n",
        "    if len(hard_negatives) == 5:\n",
        "      break\n",
        "\n",
        "  dpr_data['hard_negative_ctxs'] = hard_negatives\n",
        "\n",
        "  dpr_data_list.append(dpr_data)\n",
        "\n",
        "  del dpr_data, hard_negatives, hits\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/DPR_dev_v2.json', 'w') as fout:\n",
        "  json.dump(dpr_data_list, fout, ensure_ascii = False)"
      ],
      "metadata": {
        "id": "NOcNXbIYQuSZ"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}