{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c7755e-aa87-40e9-9797-382a6c31cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6671a687-8e47-45dd-97ce-86c66c3e0ec4",
   "metadata": {},
   "source": [
    "# Articles that have not been studied yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e695e-5b44-42a5-801e-10383555ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Partiran\\\\downloads'\n",
    "os.chdir(path)\n",
    "articles = []\n",
    "for each in os.listdir():\n",
    "    if each.endswith('pdf'):\n",
    "        articles.append(each)\n",
    "        print(each)\n",
    "        \n",
    "with open('not_studied_articles.txt','w') as tfile:\n",
    "\ttfile.write('\\n'.join(articles))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f0aa3-5ff0-4f58-b3c3-eeb484dae8ac",
   "metadata": {},
   "source": [
    "# Articles studied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f307dbb0-f79a-40be-b05d-a909abbaa7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets.pdf\n",
      "A Novel Deep Neural Network-Based System for.pdf\n",
      "A Replication Study of Dense Passage Retriever.pdf\n",
      "A short survey on end‑to‑end simple question answering systems.pdf\n",
      "An ensemble model for the SQuAD.pdf\n",
      "An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic Question Answering.pdf\n",
      "Arabic factoid Question-Answering system for Islamic sciences using normalized corpora.pdf\n",
      "Bert for Question Answering applied on Covid-19.pdf\n",
      "BioBERT.pdf\n",
      "CHALLENGES IN THE ISLAMIC QUESTION ANSWERING CORPORA.pdf\n",
      "Choose Your QA Model Wisely.pdf\n",
      "Data augmentation approaches in natural language processing.pdf\n",
      "Data Augmentation for BERT Fine-Tuning in Open-Domain Question Answering.pdf\n",
      "Data Augmentation for Biomedical Factoid Question Answering.pdf\n",
      "Deep learning-based question answering a survey.pdf\n",
      "Dutch QA with SQuaD and Ensemble.pdf\n",
      "Ensemble ALBERT and RoBERTa for Span Prediction in Question Answering.pdf\n",
      "Ensemble learning-based approach for improving generalization capability of machine reading comprehension systems.pdf\n",
      "Extractive Question-Answering on Meeting Transcripts.pdf\n",
      "Generating Answerable and Unanswerable.pdf\n",
      "GermanQuAD.pdf\n",
      "IslamicPCQA.pdf\n",
      "Luo_asu_0010E_22752.pdf\n",
      "MRC Experimenting on Vietnamese.pdf\n",
      "NAMED ENTITY RECOGNITION FOR QURANIC TEXT USING RULE BASED.pdf\n",
      "Natural language based analysis of SQuAD.pdf\n",
      "Neural Question Generation for Portugese language.pdf\n",
      "Open Domain Question Answering Based on Retriever-Reader Architecture(1).pdf\n",
      "Portuguese SQuaD.pdf\n",
      "Quranic Verses Semantic Relatedness Using AraBERT.pdf\n",
      "Retrieval Enhanced Data Augmentation for Question Answering on Privacy Policies.pdf\n",
      "Retrieving and Reading.pdf\n",
      "Simple and Effective Data Augmentation for Low Resource Machine Reading.pdf\n",
      "SINA-BERT.pdf\n",
      "SleepQA.pdf\n",
      "Transfer Learning for Closed Domain Question Answering in COVID-19.pdf\n",
      "Transformer models used for text-based question answering.pdf\n",
      "When does Further Pre-training MLM Help.pdf\n",
      "WikiPassageQA.pdf\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u2011' in position 222: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(each)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudied_articles.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tfile:\n\u001b[1;32m---> 11\u001b[0m \t\u001b[43mtfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u2011' in position 222: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\Partiran\\\\downloads'\n",
    "os.chdir(path)\n",
    "\n",
    "articles = []\n",
    "for each in os.listdir('studied'):\n",
    "    if each.endswith('pdf'):\n",
    "        articles.append(each)\n",
    "        print(each)\n",
    "        \n",
    "with open('studied_articles.txt','w') as tfile:\n",
    "\ttfile.write('\\n'.join(articles))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9902a8d-a06d-4a69-8c68-955eb020bead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
