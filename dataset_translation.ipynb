{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5NCQNvV6EttJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from operator import itemgetter\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYHoT-NcFOrs",
        "outputId": "ed9f5a0f-71b9-4e63-b80b-c91099e827f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "functions for reading datasets with jsonlines format"
      ],
      "metadata": {
        "id": "Zo6lS1-uGCxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Answer:\n",
        "    def __init__(self, dictionary) -> None:\n",
        "        self.text = dictionary[\"text\"]\n",
        "        self.start_char = dictionary[\"start_char\"]\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return {\n",
        "            \"text\": self.text,\n",
        "            \"start_char\": self.start_char\n",
        "        }  # answer_dict\n",
        "\n",
        "\n",
        "class PassageQuestion:\n",
        "    def __init__(self, dictionary) -> None:\n",
        "        self.answers = []\n",
        "        self.pq_id = dictionary[\"pq_id\"]\n",
        "        self.passage = dictionary[\"passage\"]\n",
        "        self.surah = dictionary[\"surah\"]\n",
        "        self.verses = dictionary[\"verses\"]\n",
        "        self.question = dictionary[\"question\"]\n",
        "        for answer in dictionary[\"answers\"]:\n",
        "            self.answers.append(Answer(answer))\n",
        "\n",
        "    def to_dict(self, include_answers=True) -> dict:\n",
        "        passage_question_dict = {\n",
        "            \"pq_id\": self.pq_id,\n",
        "            \"surah\": self.surah, \"verses\": self.verses,\n",
        "            \"passage\": self.passage,\n",
        "            \"question\": self.question,\n",
        "\n",
        "        }  # passage_question_dict\n",
        "        if include_answers:\n",
        "            passage_question_dict[\"answers\"] = [x.to_dict() for x in self.answers]\n",
        "\n",
        "        return passage_question_dict\n"
      ],
      "metadata": {
        "id": "kGNBFytmGA7q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_jsonl(input_path) -> list:\n",
        "    \"\"\"\n",
        "    Read list of objects from a JSON lines file.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line.rstrip('\\n|\\r')))\n",
        "    print('Loaded {} records from {}'.format(len(data), input_path))\n",
        "    return data\n",
        "\n",
        "def read_JSONL_file(file_path) -> list:\n",
        "    data_in_file = load_jsonl(file_path)\n",
        "\n",
        "    # get list of PassageQuestion objects\n",
        "    passage_question_objects = []\n",
        "    for passage_question_dict in data_in_file:\n",
        "        # instantiate a PassageQuestion object\n",
        "        pq_object = PassageQuestion(passage_question_dict)\n",
        "        passage_question_objects.append(pq_object)\n",
        "\n",
        "    print(f\"Collected {len(passage_question_objects)} Object from {file_path}\")\n",
        "    return passage_question_objects"
      ],
      "metadata": {
        "id": "lM8CZOGdGY5u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a function to writee jsonl dataset"
      ],
      "metadata": {
        "id": "-Q4V2UB6Gih8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dump_jsonl(data, output_path, append=False):\n",
        "    \"\"\"\n",
        "    Write list of objects to a JSON lines file.\n",
        "    \"\"\"\n",
        "    mode = 'a+' if append else 'w'\n",
        "    with open(output_path, mode, encoding='utf-8') as f:\n",
        "        for line in data:\n",
        "            json_record = json.dumps(line, ensure_ascii=False)\n",
        "            f.write(json_record + '\\n')\n",
        "    print('Wrote {} records to {}'.format(len(data), output_path))"
      ],
      "metadata": {
        "id": "7111Yvc2GeNu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# combining AQQAC and QRCD datasets"
      ],
      "metadata": {
        "id": "cB8jVFmNGpNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_passage_question_objects = read_JSONL_file('/content/drive/MyDrive/qrcd_v1.1_train.jsonl')\n",
        "test_passage_question_objects = read_JSONL_file('/content/drive/MyDrive/qrcd_v1.1_test.jsonl')\n",
        "dev_passage_question_objects = read_JSONL_file('/content/drive/MyDrive/qrcd_v1.1_dev.jsonl')\n",
        "aqqac_passage_question_objects = read_JSONL_file('/content/drive/MyDrive/ARCDrefined.jsonl')\n",
        "\n",
        "train_data = [dict({\"pq_id\": passage_question_object.pq_id,\n",
        "                    \"question\":passage_question_object.question,\n",
        "                    \"context\": passage_question_object.passage, \n",
        "                    \"answers\": r.text})\n",
        "              for passage_question_object in train_passage_question_objects\n",
        "              for r in passage_question_object.answers]\n",
        "\n",
        "test_data = [dict({\"pq_id\": passage_question_object.pq_id,\n",
        "                    \"question\":passage_question_object.question,\n",
        "                    \"context\": passage_question_object.passage, \n",
        "                    \"answers\": r.text})\n",
        "              for passage_question_object in test_passage_question_objects\n",
        "              for r in passage_question_object.answers]\n",
        "\n",
        "dev_data = [dict({\"pq_id\": passage_question_object.pq_id,\n",
        "                    \"question\":passage_question_object.question,\n",
        "                    \"context\": passage_question_object.passage, \n",
        "                    \"answers\": r.text})\n",
        "              for passage_question_object in dev_passage_question_objects\n",
        "              for r in passage_question_object.answers]\n",
        "\n",
        "aqqac_data = [dict({\"pq_id\": passage_question_object.pq_id,\n",
        "                    \"question\":passage_question_object.question,\n",
        "                    \"context\": passage_question_object.passage, \n",
        "                    \"answers\": r.text})\n",
        "              for passage_question_object in aqqac_passage_question_objects\n",
        "              for r in passage_question_object.answers]\n",
        "\n",
        "data = train_data + test_data + dev_data + aqqac_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfujKE2KGws6",
        "outputId": "22ebd06c-d0fa-43c9-9c8e-beef7aeddc5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 710 records from /content/drive/MyDrive/qrcd_v1.1_train.jsonl\n",
            "Collected 710 Object from /content/drive/MyDrive/qrcd_v1.1_train.jsonl\n",
            "Loaded 274 records from /content/drive/MyDrive/qrcd_v1.1_test.jsonl\n",
            "Collected 274 Object from /content/drive/MyDrive/qrcd_v1.1_test.jsonl\n",
            "Loaded 109 records from /content/drive/MyDrive/qrcd_v1.1_dev.jsonl\n",
            "Collected 109 Object from /content/drive/MyDrive/qrcd_v1.1_dev.jsonl\n",
            "Loaded 732 records from /content/drive/MyDrive/ARCDrefined.jsonl\n",
            "Collected 732 Object from /content/drive/MyDrive/ARCDrefined.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separating factoid data from non-factoid data"
      ],
      "metadata": {
        "id": "XQ_GzruDHfRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "marks = [ 'دلیل','هل ', 'كيف ', 'لماذا', 'ما هو سبب', 'فلماذا', 'ما هي الدلائل', 'ما الدلائل', 'الدلائل', 'الدليل', 'ما سبب']\n",
        "\n",
        "non_factoid = []\n",
        "factoid = []\n",
        "\n",
        "for each in data:\n",
        "  question = each['question'].replace('هلك', '')\n",
        "  question = question.replace('أهل', '')\n",
        "  constraint = any(ele in question for ele in marks)\n",
        "  if constraint:\n",
        "    non_factoid.append(each)\n",
        "  else:\n",
        "    factoid.append(each)"
      ],
      "metadata": {
        "id": "8S7IpCRrHOFh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = '/content/drive/MyDrive/nonfactoid.jsonl'\n",
        "dump_jsonl(non_factoid, output_path)"
      ],
      "metadata": {
        "id": "UxGeM9a5HnQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = '/content/drive/MyDrive/factoid.jsonl'\n",
        "dump_jsonl(factoid, output_path)"
      ],
      "metadata": {
        "id": "4NazAiynHtc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "installing Google translate python API"
      ],
      "metadata": {
        "id": "9KgZx5iMH0u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq4-9GTNHwyd",
        "outputId": "5ef425d7-11f8-4cc3-d26a-093669fbc19f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.10.1-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.9/dist-packages (from deep-translator) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.9/dist-packages (from deep-translator) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2022.12.7)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "translator = GoogleTranslator(source='ar',target='fa')\n",
        "\n",
        "with open('/content/drive/MyDrive/makarem.xml', 'r', encoding=\"utf8\") as f:\n",
        "    content = f.read() \n",
        "\n",
        "soup= BeautifulSoup(content, 'xml') \n",
        "\n",
        "not_complete_list = []\n",
        "\n",
        "complete_list = []\n",
        "\n",
        "for qa in factoid:\n",
        "  farsi_question = translator.translate(qa['question'])\n",
        "  farsi_answer = translator.translate(qa['answers'])\n",
        "  chapter = qa['pq_id'].split(\":\", 1)[0]\n",
        "  verses = (qa['pq_id'].split(\":\", 1)[1]).split('_')[0]\n",
        "  print(qa['pq_id'])\n",
        "  start_verse = int(verses.split('-')[0])\n",
        "  end_verse = int(verses.split('-')[1])\n",
        "\n",
        "  farsi_passage = ''\n",
        "  for i in range(start_verse, end_verse+1):\n",
        "    verse_id = 's'+ str(chapter) + '.' + str(i)\n",
        "    translated_verse = soup.find(id=verse_id).contents[0]\n",
        "    farsi_passage += translated_verse\n",
        "    farsi_passage += ' '\n",
        "\n",
        "  new_data = {}\n",
        "  new_data['context'] = farsi_passage\n",
        "  new_data['pq_id'] = qa['pq_id']\n",
        "  new_data['question'] = farsi_question\n",
        "  \n",
        "  if farsi_answer in farsi_passage:\n",
        "    new_data['answers'] = farsi_answer\n",
        "    complete_list.append(new_data)\n",
        "\n",
        "  else:\n",
        "    new_data['answers'] = qa['answers']\n",
        "    not_complete_list.append(new_data)"
      ],
      "metadata": {
        "id": "9wKdmsmkH67_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}